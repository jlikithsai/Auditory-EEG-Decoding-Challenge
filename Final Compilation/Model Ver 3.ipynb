{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2eb8f0d6-ed24-4644-a643-5311bb41e391",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import glob\n",
    "# import json\n",
    "# import logging\n",
    "# import sys\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import random\n",
    "# def batch_equalizer_fn(args):\n",
    "#     eeg = args[0]\n",
    "# #     print(\"eegshape=\",eeg.shape)\n",
    "#     num_stimuli = len(args) - 1\n",
    "#     # repeat eeg num_stimuli times\n",
    "#     new_eeg = torch.cat([eeg] * num_stimuli, dim=0)\n",
    "#     all_features = [new_eeg]\n",
    "# #     print(\"all_features=\",all_features[0].shape)\n",
    "\n",
    "#     # create args\n",
    "#     args_to_zip = [args[i::num_stimuli] for i in range(1, num_stimuli + 1)]\n",
    "# #     print(\"args_to_zip=\",args_to_zip[0].shape,args_to_zip[1].shape)\n",
    "\n",
    "#     for stimuli_features in zip(*args_to_zip):\n",
    "# #         print(\"stimuli_features=\",stimuli_features[0].shape,stimuli_features[1].shape)\n",
    "#         for i in range(num_stimuli):\n",
    "#             shift = i\n",
    "#             shifted_tuple = tuple(stimuli_features[(j - shift) % len(stimuli_features)] for j in range(len(stimuli_features)))\n",
    "#             stimulus_rolled = torch.stack(shifted_tuple)\n",
    "# #             print(\"stimulus_rolled=\", stimulus_rolled.shape)\n",
    "#             # reshape stimulus_rolled to merge the first two dimensions\n",
    "#             stimulus_rolled = stimulus_rolled.view(-1, stimulus_rolled.size(2), stimulus_rolled.size(3))\n",
    "# #             print(\"stimulus_rolled1=\", stimulus_rolled.shape)\n",
    "\n",
    "#             all_features.append(stimulus_rolled)\n",
    "# #     print(\"all_features1=\",all_features)\n",
    "    \n",
    "#     labels_list = [torch.tensor([[1 if ii == i else 0 for ii in range(num_stimuli)]]) for i in range(num_stimuli)]\n",
    "#     labels = torch.cat([label.repeat(eeg.size(0), 1) for label in labels_list], dim=0)\n",
    "# #     print(\"labels=\",labels)\n",
    "# #     print(\"tuple(all_features)=\", tuple(all_features))\n",
    "\n",
    "#     return tuple(all_features), labels\n",
    "\n",
    "# def shuffle_fn(args, number_mismatch):\n",
    "#     # repeat the last argument number_mismatch times\n",
    "#     args = list(args)\n",
    "#     for _ in range(number_mismatch):\n",
    "#         args.append(args[-1][torch.randperm(args[-1].size(0))])\n",
    "#     return tuple(args)\n",
    "\n",
    "# # Function to create frames from a tensor\n",
    "# def frame_tensor(tensor, window_length, hop_length):\n",
    "#     num_frames = (tensor.size(0) - window_length) // hop_length + 1\n",
    "#     frames = torch.stack(\n",
    "#         [tensor[i * hop_length : i * hop_length + window_length] for i in range(num_frames)]\n",
    "#     )\n",
    "#     return frames\n",
    "\n",
    "# def process_eeg(original_tensors_list):\n",
    "#     reshaped_tensors = [tensor[i].view(320, 64) for tensor in original_tensors_list for i in range(tensor.size(0))]\n",
    "\n",
    "#     lists_of_tensors = [reshaped_tensors[i:i+8] for i in range(0, len(reshaped_tensors), 8)]\n",
    "#     lists_of_tensors = lists_of_tensors[:len(reshaped_tensors)//8]\n",
    "\n",
    "#     # Shuffle the lists\n",
    "#     random.shuffle(lists_of_tensors)\n",
    "\n",
    "#     final_tensors = []\n",
    "#     for chunk_of_lists in zip(*(iter(lists_of_tensors),) * 8):\n",
    "#         concatenated_tensors = torch.cat([torch.unsqueeze(tensor, 0) for sublist in chunk_of_lists for tensor in sublist], dim=0).view(64, 320, 64)\n",
    "#         final_tensors.append(concatenated_tensors)\n",
    "\n",
    "#     return final_tensors\n",
    "# def process_stimuli(original_tensors_list):\n",
    "#     reshaped_tensors = [tensor[i].view(320, 1) for tensor in original_tensors_list for i in range(tensor.size(0))]\n",
    "\n",
    "#     lists_of_tensors = [reshaped_tensors[i:i+8] for i in range(0, len(reshaped_tensors), 8)]\n",
    "#     lists_of_tensors = lists_of_tensors[:len(reshaped_tensors)//8]\n",
    "\n",
    "#     # Shuffle the lists\n",
    "#     random.shuffle(lists_of_tensors)\n",
    "\n",
    "#     final_tensors = []\n",
    "#     for chunk_of_lists in zip(*(iter(lists_of_tensors),) * 8):\n",
    "#         concatenated_tensors = torch.cat([torch.unsqueeze(tensor, 0) for sublist in chunk_of_lists for tensor in sublist], dim=0).view(64, 320, 1)\n",
    "#         final_tensors.append(concatenated_tensors)\n",
    "\n",
    "#     return final_tensors\n",
    "# class PyTorchDataGenerator(Dataset):\n",
    "#     def __init__(self, files, window_length):\n",
    "#         self.window_length = window_length\n",
    "#         self.files = self.group_recordings(files)\n",
    "\n",
    "#     def group_recordings(self, files):\n",
    "#         new_files = []\n",
    "#         grouped = itertools.groupby(\n",
    "#             sorted(files), lambda x: \"_-_\".join(os.path.basename(x).split(\"_-_\")[:3])\n",
    "#         )\n",
    "#         for recording_name, feature_paths in grouped:\n",
    "#             new_files += [sorted(feature_paths, key=lambda x: \"0\" if x == \"eeg\" else x)]\n",
    "# #         print(\"new_files=\", new_files)\n",
    "#         return new_files\n",
    "    \n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.files)\n",
    "\n",
    "#     def __getitem__(self, recording_index):\n",
    "#         data = []\n",
    "#         for feature in self.files[recording_index]:\n",
    "#             f = np.load(feature).astype(np.float32)\n",
    "# #             print(\"f_before=\", f.shape)\n",
    "#             if f.ndim == 1:\n",
    "#                 f = f[:, None]\n",
    "# #                 print(\"f_after=\", f.shape)\n",
    "#             data += [f]\n",
    "# #         print(\"data_before=\", data)\n",
    "#         data = self.prepare_data(data)\n",
    "# #         print(\"data_after=\", data)\n",
    "# #         print(\"tuple(torch.tensor(x) for x in data)=\",tuple(torch.tensor(x) for x in data))\n",
    "#         return tuple(torch.tensor(x) for x in data)\n",
    "\n",
    "#     def __call__(self):\n",
    "#         for idx in range(self.__len__()):\n",
    "#             yield self.__getitem__(idx)\n",
    "\n",
    "#             if idx == self.__len__() - 1:\n",
    "#                 self.on_epoch_end()\n",
    "\n",
    "#     def on_epoch_end(self):\n",
    "#         np.random.shuffle(self.files)\n",
    "\n",
    "#     def prepare_data(self, data):\n",
    "#         # make sure data has dimensionality of (n_samples, n_features)\n",
    "#         return data\n",
    "\n",
    "# def create_pytorch_dataset(\n",
    "#     data_generator,\n",
    "#     window_length,\n",
    "#     batch_equalizer_fn=None,\n",
    "#     frame_tensor=None,\n",
    "#     process_eeg=None,\n",
    "#     process_stimuli=None,\n",
    "#     hop_length=64,\n",
    "#     batch_size=64,\n",
    "#     number_mismatch=None,\n",
    "#     data_types=(torch.float32, torch.float32),\n",
    "#     feature_dims=(64, 1)\n",
    "# ):\n",
    "#     dataset = data_generator\n",
    "#     if frame_tensor is not None:\n",
    "#         i=0\n",
    "#         for data in dataset:\n",
    "#             dataset = [(frame_tensor(data[0], window_length, hop_length),frame_tensor(data[1], window_length, hop_length),)]\n",
    "\n",
    "\n",
    "#     if number_mismatch is not None:\n",
    "#         # map second argument to shifted version\n",
    "#         dataset = [\n",
    "#         shuffle_fn(data, number_mismatch) for data in dataset\n",
    "#     ]\n",
    "    \n",
    "#     if process_eeg is not None and process_stimuli is not None:\n",
    "#         # map second argument to shifted version\n",
    "#         dataset=[process_eeg([data[0] for data in dataset]),\n",
    "#         process_stimuli([data[1] for data in dataset]),\n",
    "#         process_stimuli([data[2] for data in dataset]),\n",
    "#         process_stimuli([data[3] for data in dataset]),\n",
    "#         process_stimuli([data[4] for data in dataset]),\n",
    "#         process_stimuli([data[5] for data in dataset])]\n",
    "#         dataset = [tuple([dataset[0][i],dataset[1][i],dataset[2][i],dataset[3][i],dataset[4][i],dataset[5][i]]) for i in range(len(dataset[0]))]\n",
    "# #         print(dataset[0][0].shape,dataset[0][1].shape,dataset[0][2].shape)\n",
    "\n",
    "#     if batch_equalizer_fn is not None:\n",
    "#         # Create the labels and make sure classes are balanced\n",
    "#         dataset = [\n",
    "#             tuple(batch_equalizer_fn(args)) for args in dataset\n",
    "#         ]\n",
    "\n",
    "#     return tuple(dataset)\n",
    "\n",
    "# window_length_s = 5\n",
    "# fs = 64\n",
    "\n",
    "# window_length = window_length_s * fs  # 5 seconds\n",
    "# # Hop length between two consecutive decision windows\n",
    "# hop_length = 64\n",
    "\n",
    "# epochs = 100\n",
    "# patience = 5\n",
    "# batch_size = 64 #fixed in the code\n",
    "# only_evaluate = True\n",
    "# number_mismatch = 4 # or 4\n",
    "\n",
    "\n",
    "\n",
    "# training_log_filename = \"training_log_{}_{}.csv\".format(number_mismatch, window_length_s)\n",
    "# data_folder = \"split_data/split_data\"\n",
    "\n",
    "# # stimulus feature which will be used for training the model. Can be either 'envelope' ( dimension 1) or 'mel' (dimension 28)\n",
    "# stimulus_features = [\"envelope\"]\n",
    "# stimulus_dimension = 1\n",
    "\n",
    "# # uncomment if you want to train with the mel spectrogram stimulus representation\n",
    "# # stimulus_features = [\"mel\"]\n",
    "# # stimulus_dimension = 10\n",
    "\n",
    "# features = [\"eeg\"] + stimulus_features\n",
    "# # print(\"features=\", features)\n",
    "# train_files = [x for x in glob.glob(os.path.join(data_folder, \"train_-_*\")) if os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in features]\n",
    "# # Create list of numpy array files\n",
    "# train_generator = PyTorchDataGenerator(train_files, window_length)\n",
    "# import pdb\n",
    "# dataset_train = create_pytorch_dataset(train_generator, window_length, batch_equalizer_fn,frame_tensor,process_eeg,process_stimuli,\n",
    "#                                   hop_length, batch_size,\n",
    "#                                   number_mismatch=number_mismatch,\n",
    "#                                   data_types=(torch.float32, torch.float32),\n",
    "#                                   feature_dims=(64, stimulus_dimension))\n",
    "\n",
    "# val_files = [x for x in glob.glob(os.path.join(data_folder, \"val_-_*\")) if os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in features]\n",
    "# val_generator = PyTorchDataGenerator(val_files, window_length)\n",
    "# dataset_val = create_pytorch_dataset(val_generator,  window_length, batch_equalizer_fn,frame_tensor,process_eeg,process_stimuli,\n",
    "#                                   hop_length, batch_size,\n",
    "#                                   number_mismatch=number_mismatch,\n",
    "#                                   data_types=(torch.float32, torch.float32),\n",
    "#                                   feature_dims=(64, stimulus_dimension))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8065f1e-650c-4bc4-8ccb-8002023fff70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "666\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 216\u001b[0m\n\u001b[0;32m    214\u001b[0m train_generator \u001b[38;5;241m=\u001b[39m PyTorchDataGenerator(train_files, window_length)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpdb\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m dataset_train \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_pytorch_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwindow_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43mframe_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprocess_eeg\u001b[49m\u001b[43m,\u001b[49m\u001b[43mprocess_stimuli\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    217\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mhop_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    218\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mnumber_mismatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mdata_types\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mfeature_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstimulus_dimension\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    222\u001b[0m val_files \u001b[38;5;241m=\u001b[39m [x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(data_folder, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_-_*\u001b[39m\u001b[38;5;124m\"\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(x)\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_-_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[0;32m    223\u001b[0m val_generator \u001b[38;5;241m=\u001b[39m PyTorchDataGenerator(val_files, window_length)\n",
      "Cell \u001b[1;32mIn[2], line 160\u001b[0m, in \u001b[0;36mcreate_pytorch_dataset\u001b[1;34m(data_generator, window_length, batch_equalizer_fn, frame_tensor, process_eeg, process_stimuli, hop_length, batch_size, number_mismatch, data_types, feature_dims)\u001b[0m\n\u001b[0;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m frame_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    159\u001b[0m     i\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m--> 160\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m [(frame_tensor(data[\u001b[38;5;241m0\u001b[39m], window_length, hop_length),frame_tensor(data[\u001b[38;5;241m1\u001b[39m], window_length, hop_length)) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(dataset)]\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m number_mismatch \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;66;03m# map second argument to shifted version\u001b[39;00m\n\u001b[0;32m    165\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    166\u001b[0m     shuffle_fn(data, number_mismatch) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m dataset\n\u001b[0;32m    167\u001b[0m ]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import tqdm\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import sys\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "def batch_equalizer_fn(args):\n",
    "    eeg = args[0]\n",
    "#     print(\"eegshape=\",eeg.shape)\n",
    "    num_stimuli = len(args) - 1\n",
    "    # repeat eeg num_stimuli times\n",
    "    new_eeg = torch.cat([eeg] * num_stimuli, dim=0)\n",
    "    all_features = [new_eeg]\n",
    "#     print(\"all_features=\",all_features[0].shape)\n",
    "\n",
    "    # create args\n",
    "    args_to_zip = [args[i::num_stimuli] for i in range(1, num_stimuli + 1)]\n",
    "#     print(\"args_to_zip=\",args_to_zip[0].shape,args_to_zip[1].shape)\n",
    "\n",
    "    for stimuli_features in zip(*args_to_zip):\n",
    "#         print(\"stimuli_features=\",stimuli_features[0].shape,stimuli_features[1].shape)\n",
    "        for i in range(num_stimuli):\n",
    "            shift = i\n",
    "            shifted_tuple = tuple(stimuli_features[(j - shift) % len(stimuli_features)] for j in range(len(stimuli_features)))\n",
    "#             print(len(shifted_tuple))\n",
    "            stimulus_rolled = torch.stack(shifted_tuple)\n",
    "#             print(\"stimulus_rolled=\", stimulus_rolled.shape)\n",
    "            # reshape stimulus_rolled to merge the first two dimensions\n",
    "            stimulus_rolled = stimulus_rolled.view(-1, stimulus_rolled.size(2), stimulus_rolled.size(3))\n",
    "#             print(\"stimulus_rolled1=\", stimulus_rolled.shape)\n",
    "\n",
    "            all_features.append(stimulus_rolled)\n",
    "#     print(\"all_features1=\",all_features)\n",
    "    \n",
    "    labels_list = [torch.tensor([[1 if ii == i else 0 for ii in range(num_stimuli)]]) for i in range(num_stimuli)]\n",
    "    labels = torch.cat([label.repeat(eeg.size(0), 1) for label in labels_list], dim=0)\n",
    "#     print(\"labels=\",labels)\n",
    "#     print(\"tuple(all_features)=\", tuple(all_features))\n",
    "\n",
    "    return tuple(all_features), labels\n",
    "\n",
    "def shuffle_fn(args, number_mismatch):\n",
    "    # repeat the last argument number_mismatch times\n",
    "    args = list(args)\n",
    "    for _ in range(number_mismatch):\n",
    "        args.append(args[-1][torch.randperm(args[-1].size(0))])\n",
    "    return tuple(args)\n",
    "\n",
    "# Function to create frames from a tensor\n",
    "def frame_tensor(tensor, window_length, hop_length):\n",
    "    num_frames = (tensor.size(0) - window_length) // hop_length + 1\n",
    "    frames = torch.stack(\n",
    "        [tensor[i * hop_length : i * hop_length + window_length] for i in range(num_frames)]\n",
    "    )\n",
    "    return frames\n",
    "\n",
    "def process_eeg(original_tensors_list):\n",
    "    reshaped_tensors = [tensor[i].view(320, 64) for tensor in original_tensors_list for i in range(tensor.size(0))]\n",
    "\n",
    "    lists_of_tensors = [reshaped_tensors[i:i+8] for i in range(0, len(reshaped_tensors), 8)]\n",
    "    lists_of_tensors = lists_of_tensors[:len(reshaped_tensors)//8]\n",
    "\n",
    "    # Shuffle the lists\n",
    "    random.shuffle(lists_of_tensors)\n",
    "\n",
    "    final_tensors = []\n",
    "    for chunk_of_lists in zip(*(iter(lists_of_tensors),) * 8):\n",
    "        concatenated_tensors = torch.cat([torch.unsqueeze(tensor, 0) for sublist in chunk_of_lists for tensor in sublist], dim=0).view(64, 320, 64)\n",
    "        final_tensors.append(concatenated_tensors)\n",
    "\n",
    "    return final_tensors\n",
    "def process_stimuli(original_tensors_list,shape):\n",
    "    reshaped_tensors = [tensor[i].view(320, shape) for tensor in original_tensors_list for i in range(tensor.size(0))]\n",
    "\n",
    "    lists_of_tensors = [reshaped_tensors[i:i+8] for i in range(0, len(reshaped_tensors), 8)]\n",
    "    lists_of_tensors = lists_of_tensors[:len(reshaped_tensors)//8]\n",
    "\n",
    "    # Shuffle the lists\n",
    "    random.shuffle(lists_of_tensors)\n",
    "\n",
    "    final_tensors = []\n",
    "    for chunk_of_lists in zip(*(iter(lists_of_tensors),) * 8):\n",
    "        concatenated_tensors = torch.cat([torch.unsqueeze(tensor, 0) for sublist in chunk_of_lists for tensor in sublist], dim=0).view(64, 320, shape)\n",
    "        final_tensors.append(concatenated_tensors)\n",
    "\n",
    "    return final_tensors\n",
    "class PyTorchDataGenerator(Dataset):\n",
    "    def __init__(self, files, window_length):\n",
    "        self.window_length = window_length\n",
    "        self.files = self.group_recordings(files)\n",
    "\n",
    "    def group_recordings(self, files):\n",
    "        new_files = []\n",
    "        grouped = itertools.groupby(\n",
    "            sorted(files), lambda x: \"_-_\".join(os.path.basename(x).split(\"_-_\")[:3])\n",
    "        )\n",
    "        for recording_name, feature_paths in grouped:\n",
    "            new_files += [sorted(feature_paths, key=lambda x: \"0\" if x == \"eeg\" else x)]\n",
    "#         print(\"new_files=\", new_files[0:2])\n",
    "        return new_files\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, recording_index):\n",
    "        data = []\n",
    "        for feature in self.files[recording_index]:\n",
    "            f = np.load(feature).astype(np.float32)\n",
    "#             print(\"f_before=\", f.shape)\n",
    "            if f.ndim == 1:\n",
    "                f = f[:, None]\n",
    "#                 print(\"f_after=\", f.shape)\n",
    "            data += [f]\n",
    "#         print(\"data_before=\", data)\n",
    "        data = self.prepare_data(data)\n",
    "#         print(\"data_after=\", data)\n",
    "#         print(\"tuple(torch.tensor(x) for x in data)=\",tuple(torch.tensor(x) for x in data))\n",
    "        return tuple(torch.tensor(x) for x in data)\n",
    "\n",
    "    def __call__(self):\n",
    "        for idx in range(self.__len__()):\n",
    "            yield self.__getitem__(idx)\n",
    "\n",
    "            if idx == self.__len__() - 1:\n",
    "                self.on_epoch_end()\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.files)\n",
    "\n",
    "    def prepare_data(self, data):\n",
    "        # make sure data has dimensionality of (n_samples, n_features)\n",
    "        return data\n",
    "\n",
    "def create_pytorch_dataset(\n",
    "    data_generator,\n",
    "    window_length,\n",
    "    batch_equalizer_fn=None,\n",
    "    frame_tensor=None,\n",
    "    process_eeg=None,\n",
    "    process_stimuli=None,\n",
    "    hop_length=64,\n",
    "    batch_size=64,\n",
    "    number_mismatch=None,\n",
    "    data_types=(torch.float32, torch.float32),\n",
    "    feature_dims=(64, 1)\n",
    "):\n",
    "    dataset = data_generator\n",
    "    \n",
    "    if frame_tensor is not None:\n",
    "        i=0\n",
    "        dataset = [(frame_tensor(data[0], window_length, hop_length),frame_tensor(data[1], window_length, hop_length)) for data in tqdm(dataset)]\n",
    "\n",
    "\n",
    "    if number_mismatch is not None:\n",
    "        # map second argument to shifted version\n",
    "        dataset = [\n",
    "        shuffle_fn(data, number_mismatch) for data in dataset\n",
    "    ]\n",
    "    \n",
    "    if process_eeg is not None and process_stimuli is not None:\n",
    "        # map second argument to shifted version\n",
    "        dataset=[process_eeg([data[0] for data in tqdm(dataset)]),\n",
    "        process_stimuli([data[1] for data in tqdm(dataset)],feature_dims[1])]\n",
    "        dataset = [tuple([dataset[0][i],dataset[1][i]]) for i in range(len(dataset[0]))]\n",
    "#         print(dataset[0][0].shape,dataset[0][1].shape,dataset[0][2].shape)\n",
    "\n",
    "    if batch_equalizer_fn is not None:\n",
    "        # Create the labels and make sure classes are balanced\n",
    "        dataset = [\n",
    "            tuple(batch_equalizer_fn(args)) for args in dataset\n",
    "        ]\n",
    "\n",
    "    return tuple(dataset)\n",
    "\n",
    "window_length_s = 5\n",
    "fs = 64\n",
    "\n",
    "window_length = window_length_s * fs  # 5 seconds\n",
    "# Hop length between two consecutive decision windows\n",
    "hop_length = 64\n",
    "\n",
    "epochs = 100\n",
    "patience = 5\n",
    "batch_size = 64 #fixed in the code\n",
    "only_evaluate = True\n",
    "number_mismatch = 4 # or 4\n",
    "\n",
    "\n",
    "\n",
    "training_log_filename = \"training_log_{}_{}.csv\".format(number_mismatch, window_length_s)\n",
    "data_folder = \"split_data/split_data\"\n",
    "\n",
    "# stimulus feature which will be used for training the model. Can be either 'envelope' ( dimension 1) or 'mel' (dimension 28)\n",
    "# stimulus_features = [\"envelope\"]\n",
    "# stimulus_dimension = 1\n",
    "\n",
    "# uncomment if you want to train with the mel spectrogram stimulus representation\n",
    "stimulus_features = [\"mel\"]\n",
    "stimulus_dimension = 10\n",
    "\n",
    "features = [\"eeg\"] + stimulus_features\n",
    "# print(\"features=\", features)\n",
    "train_files = [x for x in glob.glob(os.path.join(data_folder, \"train_-_*\")) if os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in features]\n",
    "# Create list of numpy array files\n",
    "train_generator = PyTorchDataGenerator(train_files, window_length)\n",
    "import pdb\n",
    "dataset_train = create_pytorch_dataset(train_generator, window_length, None,frame_tensor,process_eeg,process_stimuli,\n",
    "                                  hop_length, batch_size,\n",
    "                                  number_mismatch=None,\n",
    "                                  data_types=(torch.float32, torch.float32),\n",
    "                                  feature_dims=(64, stimulus_dimension))\n",
    "\n",
    "val_files = [x for x in glob.glob(os.path.join(data_folder, \"val_-_*\")) if os.path.basename(x).split(\"_-_\")[-1].split(\".\")[0] in features]\n",
    "val_generator = PyTorchDataGenerator(val_files, window_length)\n",
    "dataset_val = create_pytorch_dataset(val_generator,  window_length, None,frame_tensor,process_eeg,process_stimuli,\n",
    "                                  hop_length, batch_size,\n",
    "                                  number_mismatch=None,\n",
    "                                  data_types=(torch.float32, torch.float32),\n",
    "                                  feature_dims=(64, stimulus_dimension))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5098a9a-3f5e-496d-8797-5015fa81fc5d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdataset_train\u001b[49m:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n\u001b[0;32m      4\u001b[0m     c \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dataset_train' is not defined"
     ]
    }
   ],
   "source": [
    "c = 0 \n",
    "for i in dataset_train:\n",
    "    print(i)\n",
    "    c += 1\n",
    "    if c == 2:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e0c7730-3978-4817-bd94-44c5a7fb27ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm \n",
    "\n",
    "class ResNet34EEG(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet34EEG, self).__init__()\n",
    "        self.model = models.resnet34(pretrained=False)\n",
    "        self.model.conv1 = nn.Conv2d(64, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "class ResNet34Mel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet34Mel, self).__init__()\n",
    "        self.model = models.resnet34(pretrained=False)\n",
    "        self.model.conv1 = nn.Conv2d(10, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "resnet_eeg = ResNet34EEG().to(device)\n",
    "resnet_mel = ResNet34Mel().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d15ab-6331-433d-b5c9-56b3c68c14a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(resnet_eeg.parameters()) + list(resnet_mel.parameters()), lr=0.001)\n",
    "batch_size = 2\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for eeg_input, mel_input in tqdm(dataset_train, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        optimizer.zero_grad()\n",
    "        # print(eeg_input.shape)\n",
    "        eeg_input, mel_input = eeg_input.view(64,64,320,1).to(device) , mel_input.view(64,10,320,1).to(device)\n",
    "        output_eeg = resnet_eeg(eeg_input)\n",
    "        output_mel = resnet_mel(mel_input)\n",
    "\n",
    "        loss = torch.abs(output_eeg - output_mel).sum()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataset_train)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss}')\n",
    "\n",
    "# Save the trained models if needed\n",
    "torch.save(resnet_eeg.state_dict(), 'resnet_eeg.pth')\n",
    "torch.save(resnet_mel.state_dict(), 'resnet_mel.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f827010-17ff-4cf1-91c4-48cb5f1769aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm  \n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.resnet = models.resnet34(pretrained=False)\n",
    "        self.resnet.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(512, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "resnet_eeg_soft = CustomResNet(64)\n",
    "resnet_mel_soft = CustomResNet(10)\n",
    "\n",
    "resnet_eeg_soft = resnet_eeg_soft.to(device)\n",
    "resnet_mel_soft = resnet_mel_soft.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb32925a-84d8-4ddb-a854-fbcd73abbce6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(resnet_eeg_soft.parameters()) + list(resnet_mel_soft.parameters()), lr=0.001)\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 5\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for eeg_input, mel_input in tqdm(dataset_train, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        eeg_input, mel_input = eeg_input.view(64,64,320,1).to(device) , mel_input.view(64,10,320,1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_eeg = resnet_eeg_soft(eeg_input)\n",
    "        output_mel = resnet_mel_soft(mel_input)\n",
    "\n",
    "        # loss = torch.abs(output_eeg - output_mel).sum()\n",
    "        loss = criterion(output_eeg,output_mel)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataset_train)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss}')\n",
    "\n",
    "torch.save(resnet_eeg.state_dict(), 'resnet_eeg_soft.pth')\n",
    "torch.save(resnet_mel.state_dict(), 'resnet_mel_soft.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adb7892",
   "metadata": {},
   "source": [
    "### Speech FFR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59432b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import hilbert, butter, filtfilt\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class EnvelopeExtractor:\n",
    "    def __init__(self, speech_envelope, high_freq_cutoff, low_freq_cutoff, sampling_rate):        \n",
    "        self.speech_envelope = speech_envelope\n",
    "        self.high_freq_cutoff = high_freq_cutoff\n",
    "        self.low_freq_cutoff = low_freq_cutoff\n",
    "        self.sampling_rate = sampling_rate\n",
    "\n",
    "    def design_bandpass_filter(self):\n",
    "        nyquist = 0.5 * self.sampling_rate\n",
    "        b, a = butter(4, [self.low_freq_cutoff / nyquist, self.high_freq_cutoff / nyquist], btype='band')\n",
    "        return b, a\n",
    "\n",
    "    def extract_envelope(self):\n",
    "        b, a = self.design_bandpass_filter()\n",
    "        filtered_envelope = filtfilt(b, a, self.speech_envelope)\n",
    "        analytic_signal = hilbert(filtered_envelope)\n",
    "        amplitude_envelope = np.abs(analytic_signal)\n",
    "        return amplitude_envelope"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e1d8d5",
   "metadata": {},
   "source": [
    "### Attention Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99952c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AttentionEncoder(nn.Module):\n",
    "    def __init__(self, eeg_input_size, audio_input_size, hidden_size):\n",
    "        super(AttentionEncoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.eeg_lstm = nn.LSTM(eeg_input_size, hidden_size, bidirectional=True)\n",
    "        self.audio_lstm = nn.LSTM(audio_input_size, hidden_size, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 4, hidden_size)  # Output size matches hidden size\n",
    "        self.attention = nn.Linear(hidden_size * 4, 1, bias=False)\n",
    "\n",
    "    def forward(self, eeg_input, audio_input):\n",
    "        eeg_outputs, (eeg_hidden, eeg_cell) = self.eeg_lstm(eeg_input)\n",
    "        audio_outputs, (audio_hidden, audio_cell) = self.audio_lstm(audio_input)\n",
    "        \n",
    "        combined_outputs = torch.cat((eeg_outputs, audio_outputs), dim=2)\n",
    "        energy = self.attention(combined_outputs)\n",
    "        \n",
    "        # Apply attention independently for each sequence\n",
    "        attention_weights = torch.softmax(energy, dim=0)\n",
    "        context_vector = torch.sum(combined_outputs * attention_weights, dim=0)\n",
    "        \n",
    "        # Apply a fully connected layer for the final output\n",
    "        output = torch.tanh(self.fc(context_vector))\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# # Example usage:\n",
    "# eeg_data = torch.randn(10, 32, 64)  # EEG data shape: (seq_len, batch_size, input_size)\n",
    "# audio_data = torch.randn(1, 32, 128)  # Audio data shape: (seq_len, batch_size, input_size)\n",
    "# input_size_eeg = 64\n",
    "# input_size_audio = 128\n",
    "# hidden_size = 128\n",
    "# # Repeat audio data to match EEG sequence length\n",
    "# audio_data = audio_data.expand(10, -1, -1)  # Expand audio to (10, 32, 128)\n",
    "# ae = AttentionEncoder(input_size_eeg, input_size_audio, hidden_size)\n",
    "# output = ae(eeg_data, audio_data)\n",
    "# print(output.shape)  #the output shape is (32,128) which is the same as the batch size and hidden size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd7830d",
   "metadata": {},
   "source": [
    "### Speech FFR + Resnet\n",
    "Shivang please create the new ffr dataset in the same format as the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c44ae7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPLY BELOW CODE TO ENTIRE AUDIO DATASET THEN APPLY RESNET AS BEFORE\n",
    "\n",
    "# extractor = EnvelopeExtractor(speech_envelope=speech_envelope,\n",
    "#                               high_freq_cutoff=4000,   #freq values according to ChatGPT  \n",
    "#                               low_freq_cutoff=50,   \n",
    "#                               sampling_rate=1000)\n",
    "# speechffr_envelope = extractor.extract_envelope()\n",
    "\n",
    "stimulus_features = [\"mel\"]\n",
    "stimulus_dimension = 10\n",
    "features = [\"eeg\"] + stimulus_features\n",
    "dataset_train = create_pytorch_dataset(train_generator, window_length, None,frame_tensor,process_eeg,process_stimuli,\n",
    "                                  hop_length, batch_size,\n",
    "                                  number_mismatch=None,\n",
    "                                  data_types=(torch.float32, torch.float32),\n",
    "                                  feature_dims=(64, stimulus_dimension))\n",
    "dataset_train_ffr=[]\n",
    "for batch in dataset_train:\n",
    "    batch=list(batch)\n",
    "    eeg_input=batch[0]\n",
    "    mel_input=batch[1]\n",
    "    extractor = EnvelopeExtractor(speech_envelope=mel_input,\n",
    "                           high_freq_cutoff=4000,     \n",
    "                           low_freq_cutoff=50,   \n",
    "                           sampling_rate=1000)\n",
    "    mel_input = extractor.extract_envelope()\n",
    "    dataset_train_ffr.append((eeg_input,mel_input))\n",
    "    #Shivang Add here\n",
    "    #CREATE NEW DATASET WITH THIS MEL_INPUT AND THE EEG_INPUT let's call it dataset_train_ffr\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm  \n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.resnet = models.resnet34(pretrained=False)\n",
    "        self.resnet.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(512, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "resnet_eeg_soft = CustomResNet(64)\n",
    "resnet_mel_soft = CustomResNet(10)\n",
    "\n",
    "resnet_eeg_soft = resnet_eeg_soft.to(device)\n",
    "resnet_mel_soft = resnet_mel_soft.to(device)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(list(resnet_eeg_soft.parameters()) + list(resnet_mel_soft.parameters()), lr=0.001)\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 100\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    \n",
    "    #DATASET_TRAIN_FFR IS THE NEW DATASET WITH THE FFR ENVELOPE AND THE EEG INPUT\n",
    "    \n",
    "    for eeg_input, mel_input in tqdm(dataset_train_ffr, desc=f'Epoch {epoch+1}/{num_epochs}'):                \n",
    "        eeg_input, mel_input = eeg_input.view(64,64,320,1).to(device) , mel_input.view(64,10,320,1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_eeg = resnet_eeg_soft(eeg_input)\n",
    "        output_mel = resnet_mel_soft(mel_input)\n",
    "\n",
    "        # loss = torch.abs(output_eeg - output_mel).sum()\n",
    "        loss = criterion(output_eeg,output_mel)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataset_train)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss}')\n",
    "\n",
    "torch.save(resnet_eeg.state_dict(), 'resnet_eeg_soft.pth')\n",
    "torch.save(resnet_mel.state_dict(), 'resnet_mel_soft.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f8ba8e",
   "metadata": {},
   "source": [
    "### Resnet + Attention\n",
    "Rachit please add on to the corresponding resnet layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbc4bf6-5f2b-4968-b5bb-218ba34949fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "from tqdm import tqdm  \n",
    "\n",
    "class CustomResNet(nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(CustomResNet, self).__init__()\n",
    "        self.resnet = models.resnet34(pretrained=False)\n",
    "        self.resnet.conv1 = nn.Conv2d(input_channels, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.resnet.fc = nn.Linear(512, 10)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.resnet(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "resnet_eeg_soft = CustomResNet(64)\n",
    "resnet_mel_soft = CustomResNet(10)\n",
    "\n",
    "resnet_eeg_soft = resnet_eeg_soft.to(device)\n",
    "resnet_mel_soft = resnet_mel_soft.to(device)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(list(resnet_eeg_soft.parameters()) + list(resnet_mel_soft.parameters()), lr=0.001)\n",
    "\n",
    "batch_size = 8\n",
    "num_epochs = 100\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for eeg_input, mel_input in tqdm(dataset_train, desc=f'Epoch {epoch+1}/{num_epochs}'):\n",
    "        eeg_input, mel_input = eeg_input.view(64,64,320,1).to(device) , mel_input.view(64,10,320,1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output_eeg = resnet_eeg_soft(eeg_input)\n",
    "        output_mel = resnet_mel_soft(mel_input)\n",
    "        \n",
    "        #RACHIT UPDATE THE VALUES ACCORDINGLY AND ADD THE NEXT LAYERS\n",
    "        #UPDATED ATTENTION PART\n",
    "        \n",
    "        # input_size_eeg = 64             #change on basis of ResNet output\n",
    "        # input_size_audio = 128          #change on basis of ResNet output\n",
    "        # hidden_size = 128               #change on basis of ResNet input as this will be for the next ResNet layer\n",
    "\n",
    "        # # Repeat audio data to match EEG sequence length\n",
    "        # audio_data = audio_data.expand(10, -1, -1)  # Expand audio to (10, 32, 128)\n",
    "\n",
    "        # ae = AttentionEncoder(input_size_eeg, input_size_audio, hidden_size)\n",
    "\n",
    "        # audio_layer2 = ae(eeg_data, audio_data)\n",
    "        \n",
    "        #UPDATED ATTENTION PART\n",
    "        \n",
    "        \n",
    "        #NOW FEED THE OUTPUT OF THE ATTENTION LAYER TO THE NEXT RESNET LAYER( KEEP EEG SAME, JUST CHANGE THE AUDIO INPUT)\n",
    "        #APPLY ATTENTION LAYER SIMILAR TO BEFORE\n",
    "        #FEED THE OUTPUT TO THE FINAL RESNET LAYER\n",
    "        \n",
    "       \n",
    "        loss = criterion(output_eeg,output_mel)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    average_loss = total_loss / len(dataset_train)\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {average_loss}')\n",
    "\n",
    "torch.save(resnet_eeg.state_dict(), 'resnet_eeg_soft.pth')\n",
    "torch.save(resnet_mel.state_dict(), 'resnet_mel_soft.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
